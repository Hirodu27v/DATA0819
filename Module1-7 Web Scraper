Finding & Getting Data - Web Scraper
このビデオでは、Google ChromeのWeb Scraperというとても便利な拡張機能を使う方法をお見せします。
Chrome　Webストアから検索してダウンロードしてください。こんな感じのページです。
Web Scraperを使用すると、Webサイトから情報をスクレイピングして、独自のデータセットを作成することができます。例えば、Billboard200のウェブページを見てみると、毎週のトップ曲がリストされています。
このデータセットを取得するにはコピー＆ペーストする必要がありますが、全てをコピー＆ペーストすることはできないので、（このままでは）全ての情報を手で入力するしかないのです。
テーブルではないので、importHTMLは機能しません。私たちの目標は、全ての情報をテーブルに取り込み、このようにすることです。
この情報をスクレイピングした時点でのアーティスト、順位、URL、さらにはスクレイピングした週までもが列に入っています。
これを実現するために必要なのは特定のパターンを見つけて情報を収集し、データセットにすることです。
Webページを見てみましょう。白いボックスが見えますよね。パターンを見つけて情報を集めるにはどうしたらいいのでしょうか。
このボックスは200個あって、必要な情報はこの中に納められています。曲の名前はここ、アーティスト名はここ、順位はここにあって、そうそう、アルバムの画像もここにあります。週の情報もここにありました。
ここでは、Web Scraperを使用してこれらの情報をすべて収集します。
まず、Webインスペクタメニューにアクセスします。任意の場所を右クリックし、「検証」を選ぶとタブ右端にWeb Scraperという新しいオプションが表示されます。
ここにある三つの点から"Dock to bottom"を選択すると画面下部にWeb scraperが展開されます。上部には三つの選択肢があり、これがsitemapです。
これはスクレイピングを開始するロボットとルーチンです。「新しいサイトマップの作成」または「新しいサイトマップのインポート」というオプションがあります。
「サイトマップの作成」を選択し、Billboard200という名前を付けて、URLを入力します。
次に、新しいSelectorを追加します。Selectorは、Webページ上の要素を識別できる情報です。
ここをクリックして青いボタンをクリックするだけで、この白いボックスがどこにあるのかWeb Scraperに指示できます。そうですね。
Web Scraperに「Web Scraper、このページで200個のボックスを検索」と指示し、ボックスのどこに情報があるかを伝えスクレイピングできるようにします。
ここではこれをBOXと名付けましょう。これはWebページの要素なので、ここでのタイプはElementです。
「Select」をクリックすると、 「multiple」のボックスが表示されます。このチェックボックスをオンにします。
マウスを動かし始めると、Webページの要素がすべて表示され、どの部分を選択できるかが示されます。
右下の隅にカーソルを合わせるとボックス全体が緑になります。クリックすると赤になります。
次のボックスも同じようにして選択すると、Web scraperは他のボックスがどこにあるかを推測しようとします。
でも21個目で止まってしまいます。21個目のボックスも選択すると、ページ内の200個のボックスが全て識別されていますね。
ここに、これらのすべてのボックスを記述するselectorが入っていることがわかります。
気にしなくても、自動的にselectorは入力されます。 "Done selecting!" をクリックします。データをプレビューすることもできます。
まだスクレイピングしたデータはありませんが、要素のプレビューも可能です。ボックスの全てが選択されていることが確認できます。
保存してBOX（というselector）ができました。
ボックスをキャプチャするルーチンができましたが、次に200個のボックスの中の全ての情報をキャプチャするルーチンが必要です。
マウスをここに置くとボックスが灰色で強調表示されます。ボックスをクリックするとここで選択した一般的なボックスが表示されます。
Web Scraperに「ボックスの場所は教えたから、次はそれぞれのボックスの中の情報をキャプチャしてデータセットを作るように」と指示します。
それでは、曲名とアーティスト名、順位とアルバム画像のURLを取得します。"Add new selector"はBOXの中で行うことに注意してください。
新しいselectorを選択し、これを「song」と名付けます。「選択」をクリックし、曲名を選択します。
曲名が識別されたら、"Done selecting!"をクリックします。データをプレビューすると、全ての曲名がここに入っているので問題ありません。
selectorを保存します。新しいものを追加します。青のボタンをクリック。アーティスト名です。これはテキストなのでtypeはテキストを選びます。
selectorは「a」です。"Done selecting!" 。ここにはキャプチャされた全てのアーティスト名が表示されます。「保存する」をクリックします。
次は順位です。これもテキストです。"Select"をクリックし、順位をクリックします。そしてまた"Done selecting!"。ここで疑問に思われるかもしれませんが、「multiple」をオンにするとどうなるでしょう。
そうですね。先ほどは複数のボックスをキャプチャするために使いました。ここでは順位は黄色のボックスの中に一つしかありません。
複数のエレメントを選択する場合のみmultipleを選択します。この場合は順位も曲名もアーティスト名も一つしかないので、その必要はありません。さて、またselectorを保存します。
次はアルバム画像です。これはイメージなのでselector名も「image」としましょう。アルバム画像を選択し、"Done selecting!"をクリックしてから保存します。
 "_root"に戻って（「BOX」の）データをプレビューすると、必要なものがほぼ全てそろっているのが分かります。
曲名、アーティスト名、順位と画像のURLを手に入れました。しかしもう一つ、日付が必要だと思いませんか？日付はこのトップのところにあります。
次にやるべきは各列に日付をつけることです。
今はBOXの中にいるので"_root" に戻りましょう。そして新たに日付のselectorを作ります。
これもテキストのselectorです。ハイライト表示された部分を選んで "Done selecting!"
しかし、余計な情報も含まれています。必要なのは "August 31st"と2019だけです。どうやって抽出すればいいのでしょうか？
ここで正規表現を使いますが、詳細には触れません。「正規表現とは」とか優れたチュートリアル、よくできたクラスをぜひGoogleで検索してみてください。
特にプログラミングでは非常に強力な概念です。ありがたいことにWeb Scraperには"Regex"という正規表現を適用するフィールドがあります。
ここでは、日付だけを抽出するためのパターンを見つけたいと思います。"August 31st, 2019" といったような数字とコンマから構成されるテキストを抽出するようにします。
では、このオンラインの正規表現テスターを見てみましょう。8月31日の週とその前後の週、日付検索のストリングがあります。正規表現には三つの要素があります。7
最初は "\w+"で、バックスラッシュはトークンのようなものです。ｗは任意の文字で、a, b, c, dでも任意の数字でもかまいません。
So it matches anything that happens
here and the "+" means any character to just one character, or an infinite number of
characters until it hits the space. But it's not only a space, it's a space that comes before
like a "\d" which means digits. Right. So it matches a digit equals to 0-9, and the plus
means matches between 1 and unlimited time, so any number of digits. So, it's a word with
any number of characters, a space that comes before any number of digits, and then there
is a comma, and then a space, and then four digits. That's what this means here.
And if you copy this here to the regular expression field in a web scraper and you do a
data preview you see that it extracts everything out and then it relieves only the August
31st, 2019. And that's exactly what we want. I'm gonna save this selector here and then
we're all done. So we're gonna go here. Here you can select other options, you can see
the selector graph where you see the "_root" and then all of the other selectors and then
the relation between them.
It can get fairly complicated depending on the complexity of the page. You can edit the
metadata, the name of the site, or the URL. You can also scrape. You can browse the
scrape data and you can even export this site map. So this is like a Jason that you can
string, that you can export and use in other computers, or send to a friend that will load the
same scraping routine in other computers. Or you can tweak it a bit to change the website.
So there is a way to export.
And then you can import as well and then you can export the data that you scraped as
"CSV". So we're gonna go ahead and scrape here on this option. So it gives you two, two
options. The "request interval" which is like the amount of time that you will wait until it
does a request to the website. And two ms, which is two seconds is good practice. So you
don't wanna be hammering the site with requests, so we might look suspicious. The
webmaster might think that you're trying to take this website, down so we don't want it
dead. You want to use this option responsibly.
And then there's the page load delay. The page load delay is the amount of time that web
scraper waits for the page to load and then to scrape the data. So you might want to give
the Web site some time to load the data and then to scrape it to finish, so that you can
guarantee that all elements load before you start capturing information with that.
So two and two seconds are good numbers to start but you might tweak it depending on
your case. So you click here on "start scraping", it opens a window. It waits two seconds to
load the page and then you wait two seconds to do the request, and then it scrapes the
data. And if you click here on "refresh" it will load all of the data that it just scraped, and
voila, we have here the same metadata that Web Scraper adds. This is the web scraper
I.D. for each one of the records, you have to start URL. Here we have the data that we
really want to scrape which song, the artist, the position, the image URL, and also the date
that was applied to every record here.
So now we can go ahead and click here and export this site at CSV. And when you click
here you're going to download the CSV file to your computer, and then you can import the
CSV file to any other spreadsheet application to start analyzing or cleaning, or editing, or
building your dataset. So that's it for Web Scraper. So, go to the Chrome Web Store,
download the extension and start scraping.
