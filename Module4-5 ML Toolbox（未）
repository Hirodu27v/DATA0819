ML Toolbox
さて、記事を書くための機械学習モデルを構築するために私が使っているツールを紹介しましょう。そのうちの一つがDocument Cloudです。
これは完全に無料で、オープンソースのツールであり、テキスト分析を手助けしてくれます。
Document Cloudのデモ操作をしてみます。 多くの組織が文書をPDFでアップロードしています。他の人がアップロードしたファイルも見ることができます。
ジェームズ・コミーFBI前長官のメモを見てみましょう。ツール内のさまざまな文書にメモを残すことができますが、「エンティティ（管理対象）の抽出」という便利な機能もあります。
エンティティとは個人、場所、組織の名前といったようなもので、Document Cloudはヨーロッパにある組織なら識別することができます。例えば、トランプタワーは人の名前ではありません。
OK.オバマは人物であり、文書内の異なる箇所で言及されています。組織についても同様です。Document Cloudはホワイトハウスが組織であることを認識しています。このように、Document Cloudは、文書を分析するのに便利な、無料かつオープンソースのツールです。
機械学習にはさまざまなアプローチがありますが、私はそれを三つのカテゴリーに分けてみました。左側にDIYのツールがあります。これらは無料で使うことができ、ほとんどがオープンソースです。
例えば、PythonとRという二つの異なるプログラミング言語で使用するライブラリなどがあります。機械学習とデータサイエンスをしっかり学びたいなら、これが王道です。
おそらく高度なプログラミング能力を必要とします。先ほど触れたような機械学習の分析をしたければ、データサイエンスやモデルとは何か、どのように機能するのか、どのようにモデルを訓練するのかなどについて積極的に学ばなければなりません。
ただし、アルゴリズムを全てローカルのコンピューター上で実行でき、誰かにお金を払ったり、何らかのサービスに登録したりしなくてもよいという利点があります。
これがDIYのルートですね。最もカスタマイズが可能な方法です。任意のタイプのモデルを構築することができますが、少し手間がかかります。プログラミングとデータサイエンスを習得しなければなりません。
それは時間がかかるので、ここでは取り上げません。右の二つ、Cloud AutoMLとMLAPIについて話しましょう。一番右のMLAPIは、自分のモデルを非常に具体的な、例えば偵察機を特定するといった用途ではなく、音声の書き起こしや、共通の物体や画像の識別といった一般的な機械学習タスクを想定したものです。
これには、他の人が訓練したモデルを使うことができます。驚くべきことに、ここではGoogleのモデル、機械学習APIが使えます。
APIを呼び出すために必要なプログラミングの知識が少しばかり必要です。プログラマーとともに作業してもよいでしょう。とても使いやすく、ツールを利用するためにデータサイエンスを深く理解する必要はありません。
マイナス面は、（サービスを提供する）会社に仕事を左右されることです。あなたは全てを所有することはできません。
サービスは有料で、分析はクラウドで行われます。アプリケーションによっては、できないこともあります。
そしてもう一つ、ミドル・クラウドのカテゴリーのAutoMLがあります。
これもGoogleのツールです。ジャーナリストにこれを紹介できることにワクワクしています。というのも、カスタムモデルを作ることができるからです。
特定のタスクに対応し、誰でも使いやすいものです。カスタムモデルを構築するために高度なプログラミング知識は不要で、場合によっては全くプログラミングができなくてもよいのです。
繰り返しになりますが、これはGoogleのツールで、有料です。そしてクラウド上で動作します。しかしそれ以外は、とても便利です。
まずAPIについて説明します。Googleは、機械学習の一般的なタスク (音声からテキスト、テキストから音声) に対処する、事前に訓練されたモデルを数多く用意しています。
これは書き起こしとその逆、音声合成です。Cloud Visionというのもあります。
写真の機械学習の箇所で説明した、感情、画像、画像内のオブジェクトの識別、テキストの抽出などの多くの機能を実行できます。
自然言語については追ってお話しします。ビデオインテリジェンスを使用すると、容易にビデオが分析でき、たとえば、ビデオにキャプションを付けることができます。
translation APIは言語間の翻訳を可能にします。その一つであるCloud Natural Language Toolのデモをお見せします。
このページでAPIを試すことができます。このツールで最初に実行できるのはエンティティ分析です。先ほどお話ししたようにDocument Cloudはエンティティ解析もできるのです。
ご覧のように組織、消費財、場所、人、住所、イベント、価格、数値など様々なものを識別することができます。これは非常に便利な機能です。
例えば書式の決まった文書を解析して人の電話番号や住所を抽出したい場合などです。政治記事を集めてオバマとトランプ、そして彼らの交錯の仕方についてまとめたいとき、エンティティ抽出はとても便利です。
Sentiment Analysis APIは、人々がエンティティについて肯定的に言っているのか否定的に言っているのかを見ることができます。
Twitter上で政治家候補について人々がどう感じているかを知りたければ、このAPIをTweetsに適用することができます。もちろん、機械学習は確率論的（な判断を下すので）完璧ではありません。
皮肉を真に受けるかもしれませんから、心して使ってくださいね。このツールは構文解析を行い、音声の一部を識別することや、テキストのブロックを分類することもできます。
学習済みモデルですね。これは機械学習を簡単に始めることができる方式です。しかし、例えば琥珀の違法な採掘を発見するといった特定のタスクがあるとしましょう。
違法採掘の衛星写真を識別する、すぐに使えるツールはありませんから、自前のビジョンモデルを構築する必要があります。
DIYでもできます。しかし、Cloud AutoMLは、カスタムモデルの構築がとても簡単にできます。ただ、トレーニングデータを大量に提供しなければならないため、琥珀の衛星画像や、違法と見なされるものとそうでないものを用意しなければなりません。
全てのトレーニングデータをクラウドにアップロードすると、AutoMLがモデルを構築します。
APIを使って予測を行うことも、ツール内で予測を行うこともできます。例を挙げてみましょう。これがGoogle Cloud Platformです。私はAutoML Visionというツールを使っています。これは、ビジョン関連のカスタムモデルを構築するためのツールです。そして物体検出モデルを構築します。画像の中に何があるかを識別するだけでなく、識別したものの位置に小さなボックスを配置します。
衛星画像で飛行機を識別するモデルを作ってみましょう。偵察機ではなく普通の飛行機です。すべての衛星画像に写っている飛行機には、注釈が、つまり飛行機の周囲にボックスがあります。
ラベルがついていないものがあったら、こうして新しく飛行機を追加します。大量のラベル付きデータが必要です。この場合161枚のラベル付き画像があります。ラベル付きのモデルを構築するために必要な画像の数は作業の複雑さに比例します。
飛行機を特定するのは比較的簡単なので、肺炎の特定より少ない例で済むかもしれません。多少の差はありますが、カテゴリーごとに少なくとも数百枚は必要です。

Now, training a custom model is the easy part. You just click "train new model." Then rename the model. You can optimize it for having faster predictions or higher accuracy. Since we're journalists and we're probably not building real-time apps, we probably want to optimize for accuracy. We don't really care how long the predictions take to make. Then you can set a budget for how long you want the model to run. So again, this is a paid for thing, but I should also add that you can get a bunch of Google Cloud credits for free if you want to play around with this. And also the types of projects that you would be doing as a journalist are not the ones that are going to cost a lot. You're probably training a model once and making a couple of predictions. So you don't have this high-bandwidth prediction making, so this really shouldn't be a bottleneck cost. So you hit "train," and it takes about three or four hours to build the model. Then you can evaluate how well the model has done in this evaluate tab, here. You can see "precision recall." Recall is how good was the model at identifying all of the planes and not missing anything. And precision is how good was the model at not mistaking things for planes, so not mislabeling things that weren't planes as planes. And then you can make predictions on new satellite data right here from within the UI. So I just have this picture on my desktop of Princeton Airport. I just took it from Google Maps, and it has these unlabeled planes. And let's see how well the model can do. There you go. You can see the model identified lots of different planes in this photo. It also missed. Well, let's see. I honestly can't tell if there are planes here, but it missed a couple of planes. 
But I think that's good for you to see because, again, the remodels are usually pretty good, but not perfect. So it's important to keep in mind what you're going to do when they make an error. So I just showed you Cloud AutoML vision. This helps you make custom photos, images, models. But you could also make a custom model like this on tabular data, like BuzzFeed planes on text. You could even classify videos or improve translation models. So there are lots of different offerings. But anyway, this is my summary of the different tools that I think are the easiest to get started with machine learning. And definitely let me know if you try any of them and find they're good one way or the other.

